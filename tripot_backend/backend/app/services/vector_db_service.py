# app/services/vector_db_service.py (ìƒˆ ì´ë¦„ìœ¼ë¡œ ì €ì¥)

import uuid
import time
import asyncio
from pinecone import Pinecone, ServerlessSpec

from app.core.config import settings
from . import ai_service # ê°œì„ ëœ ai_serviceë¥¼ ì„í¬íŠ¸

# Pinecone í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë° ì¸ë±ìŠ¤ ì—°ê²°
try:
    pc = Pinecone(api_key=settings.PINECONE_API_KEY)
    
    if settings.PINECONE_INDEX_NAME not in pc.list_indexes().names():
        print(f"Pinecone ì¸ë±ìŠ¤ '{settings.PINECONE_INDEX_NAME}'ê°€ ì—†ìœ¼ë¯€ë¡œ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        pc.create_index(
            name=settings.PINECONE_INDEX_NAME, 
            dimension=1536,
            metric="cosine", 
            spec=ServerlessSpec(cloud="aws", region="us-east-1")
        )
    
    index = pc.Index(settings.PINECONE_INDEX_NAME)
    print(f"âœ… Pinecone '{settings.PINECONE_INDEX_NAME}' ì¸ë±ìŠ¤ì— ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"âŒ Pinecone ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    index = None


async def create_memory_for_pinecone(user_id: str, current_session_log: list[str]):
    """ì„¸ì…˜ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ Pineconeì— ê¸°ì–µì„ ì €ì¥í•©ë‹ˆë‹¤."""
    if not index:
        print("Pinecone ì¸ë±ìŠ¤ê°€ ì—†ì–´ ê¸°ì–µì„ ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    if not current_session_log: return
    print(f"ğŸ§  [{user_id}] ë‹˜ì˜ ì„¸ì…˜ ê¸°ì–µ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    memory_text = ""
    memory_type = ""

    if len(current_session_log) < 4:
        # ì§§ì€ ëŒ€í™”ëŠ” ì›ë¬¸ ê·¸ëŒ€ë¡œ ì €ì¥
        print("-> ì§§ì€ ëŒ€í™”ë¡œ íŒë‹¨, 'utterance' íƒ€ì…ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.")
        memory_text = "\n".join(current_session_log)
        memory_type = 'utterance'
    else:
        # ê¸´ ëŒ€í™”ëŠ” ìš”ì•½í•´ì„œ ì €ì¥ (ë™ë£Œë¶„ì˜ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ë°©ì‹ ì ìš©)
        print("-> ê¸´ ëŒ€í™”ë¡œ íŒë‹¨, 'summary' íƒ€ì…ìœ¼ë¡œ ìš”ì•½ ìƒì„±í•©ë‹ˆë‹¤.")
        conversation_history = "\n".join(current_session_log)
        
        summary_system_message = """
        ë‹¹ì‹ ì€ ì‚¬ìš©ì ëŒ€í™” ê¸°ë¡ì„ ë¶„ì„í•˜ì—¬ í•µì‹¬ì ì¸ ê¸°ì–µì„ ì¶”ì¶œí•˜ê³  ìš”ì•½í•˜ëŠ” AIì…ë‹ˆë‹¤.
        ë‹¤ìŒ ëŒ€í™” ê¸°ë¡ì—ì„œ ì‚¬ìš©ìì˜ ì¤‘ìš”í•œ ê²½í—˜, ê°ì •, ë°˜ë³µë˜ëŠ” ì£¼ì œ, íŠ¹ì´ì‚¬í•­ ë“±ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.
        ìš”ì•½ëœ ë‚´ìš©ì€ ë‹¤ë¥¸ AIê°€ ì‚¬ìš©ìì˜ ê³¼ê±°ë¥¼ íšŒìƒí•˜ê³  ëŒ€í™”ì— í™œìš©í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
        ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ê³ , ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”. (ì˜ˆ: "ì‚¬ìš©ìëŠ” ì˜¤ëŠ˜ ë³‘ì›ì— ë‹¤ë…€ì˜¨ ê²½í—˜ì— ëŒ€í•´ ì´ì•¼ê¸°í–ˆìŠµë‹ˆë‹¤.")
        ê·œì¹™: ì§€ëª…, ì¸ëª… ë“± ëª¨ë“  ê³ ìœ ëª…ì‚¬ëŠ” ë°˜ë“œì‹œ í¬í•¨ì‹œì¼œì•¼ í•©ë‹ˆë‹¤.
        """
        summary_user_message = f"--- ëŒ€í™” ë‚´ìš© ---\n{conversation_history}\n-----------------\n\ní•µì‹¬ ê¸°ì–µ ìš”ì•½:"
        
        messages_for_summary = [
            {"role": "system", "content": summary_system_message},
            {"role": "user", "content": summary_user_message}
        ]

        memory_text = await ai_service.get_ai_chat_completion(
            messages=messages_for_summary, # ê°œì„ ëœ í•¨ìˆ˜ì— messages ë¦¬ìŠ¤íŠ¸ ì „ë‹¬
            max_tokens=200,
            temperature=0.3
        )
        memory_type = 'summary'

    print(f"ğŸ“ ìƒì„±ëœ ê¸°ì–µ (íƒ€ì…: {memory_type}): {memory_text}")
    embedding = await ai_service.get_embedding(memory_text)
    
    vector_to_upsert = {
        'id': str(uuid.uuid4()), 
        'values': embedding,
        'metadata': {
            'user_id': user_id, 
            'text': memory_text, 
            'timestamp': int(time.time()), 
            'memory_type': memory_type
        }
    }
    await asyncio.to_thread(index.upsert, vectors=[vector_to_upsert])
    print(f"âœ… [{user_id}] ë‹˜ì˜ ìƒˆë¡œìš´ ê¸°ì–µì´ Pineconeì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


async def search_memories(user_id: str, query_message: str, top_k: int = 5) -> str:
    """ê³¼ê±° ê¸°ì–µì„ ê²€ìƒ‰í•˜ê³ , ê´€ë ¨ë„ì™€ ìµœì‹ ì„±ì„ ê³ ë ¤í•˜ì—¬ ìµœì¢… ê¸°ì–µ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not index:
        print("Pinecone ì¸ë±ìŠ¤ê°€ ì—†ì–´ ê¸°ì–µì„ ê²€ìƒ‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return ""
        
    query_embedding = await ai_service.get_embedding(query_message)
    results = await asyncio.to_thread(
        index.query, 
        vector=query_embedding, 
        top_k=top_k, 
        filter={'user_id': user_id}, 
        include_metadata=True
    )
    
    if not results['matches']:
        return ""

    now = int(time.time())
    ranked_memories = []
    time_decay_factor = 30 * 24 * 60 * 60  # 30ì¼

    for match in results['matches']:
        similarity_score = match['score']
        metadata = match.get('metadata', {})
        timestamp = metadata.get('timestamp', now)
        
        recency_score = max(0, (timestamp - (now - time_decay_factor)) / time_decay_factor)
        final_score = (similarity_score * 0.7) + (recency_score * 0.3)
        ranked_memories.append({'text': metadata.get('text', ''), 'score': final_score})
        
    ranked_memories.sort(key=lambda x: x['score'], reverse=True)
    top_memories = [item['text'] for item in ranked_memories[:3]]
    
    print(f"ğŸ” [{user_id}] ë‹˜ì˜ ê³¼ê±° ê¸°ì–µ {len(top_memories)}ê°œë¥¼ ê²€ìƒ‰í–ˆìŠµë‹ˆë‹¤.")
    return "\n".join(top_memories)